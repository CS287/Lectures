{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24\ttacos\tplease\t\n",
       "\t\n"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--assignment and printing \n",
    "\n",
    "a, b = 24, \"tacos\" -- can assign tuple-style, like in python \n",
    "c = 'please' -- can enclose string literals in single or double quotes\n",
    "print(a,b,c,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tacos, please\t\n"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- string concatenation\n",
    "\n",
    "d = b .. ', ' .. c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48\t576\t0\t\n",
       "\t\n",
       "\n"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " -- syntax similar to MATLAB\n",
    "print(2*a, a^2, a%2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4285714285714\t\n",
       "\t\n",
       "\n"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- note that all numbers are implicitly floats/doubles!\n",
    "print(a/7, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3\t4\t\n",
       "\t\n",
       "\n"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- if you want to ensure you get something integral, use math.ceil or math.floor\n",
    "print(math.floor(a/7), math.ceil(a/7), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t44\t\n",
       "\t\n",
       "\n"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- some other useful math functions\n",
    "print(math.min(1,22,44), math.max(1,22,44), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\t\n",
       "2\t\n"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- while loops are enclosed in while-do-end blocks\n",
    "i = 1\n",
    "while i < 3 do\n",
    "    print(i)\n",
    "    i = i + 1 -- N.B. no 'i += 1' or 'i++' syntax in Lua \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3\t\n",
       "4\t\n",
       "5\t\n",
       "\t\n"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- for-loops iterate over a range of numbers, INCLUSIVE!\n",
    "for i = 3, 5 do\n",
    "    print(i)\n",
    "end\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10\t\n",
       "6\t\n",
       "2\t\n"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- like in python, you can specify the step size with a 3rd loop argument\n",
    "for i = 10, 1, -4 do\n",
    "    print(i)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "even and nonzero!\t\n"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- conditional statements go in if-then-elseif-else-end blocks\n",
    "val = 24\n",
    "\n",
    "if val == 0 then\n",
    "    print(\"zero!\")\n",
    "elseif val%2 == 0 then\n",
    "    print(\"even and nonzero!\")\n",
    "elseif val ~= 13 then           -- N.B. Lua uses '~=' to mean '!='; also works for strings!\n",
    "    print(\"odd and not 13!\")\n",
    "else\n",
    "    print(\"everything else!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truth and Falsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "second!\t\n"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- nil and false evaluate to false\n",
    "a, b =  nil, false\n",
    "\n",
    "-- everything else evaluates to true\n",
    "c, d = \"taco\", 0\n",
    "\n",
    "if a or b then\n",
    "    print(\"first!\")\n",
    "elseif c and d then\n",
    "    print(\"second!\")\n",
    "else\n",
    "    print(\"third!\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2\t\n",
       "\t\n",
       "3\t\n",
       "\t\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- 'and' and 'or' have interesting side effects; allow for 'ternary if' as follows:\n",
    "val2 = a and 1 or 2 -- a is falsey, so we get 2\n",
    "print(val2, \"\\n\")\n",
    "\n",
    "val3 = c and 3 or 4 --  c is truthy, so we get 3\n",
    "print(val3, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34\t\n",
       "\t\n",
       "23\t\n",
       "\t\n"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- local vs global variables\n",
    "\n",
    "var = 22 -- global\n",
    "\n",
    "function f1()\n",
    "    local var = 33 -- N.B. local variables generally lead to faster code! \n",
    "    return var + 1\n",
    "end\n",
    "\n",
    "print(f1(),\"\\n\")\n",
    "\n",
    "function f2()\n",
    "    return var + 1\n",
    "end\n",
    "\n",
    "print(f2(),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321\t\n",
       "\t\n",
       "2\t\n",
       "\t\n",
       "20\t\n",
       "\t\n",
       "0\t\n",
       "\t\n",
       "321\t\n",
       "\t\n"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- default and extra arguments\n",
    "\n",
    "function encodeDigits(a,b,c)\n",
    "    local a = a or 0 -- common convention for specifying default args\n",
    "    local b = b or 0\n",
    "    local c = c or 0\n",
    "    assert(a >= 0 and a < 10)\n",
    "    assert(b >= 0 and b < 10)\n",
    "    assert(c >= 0 and c < 10)    \n",
    "    return a*1 + b*10 + c*100\n",
    "end\n",
    "\n",
    "print(encodeDigits(1,2,3),\"\\n\") -- no defaults used\n",
    "print(encodeDigits(2),\"\\n\") -- defaults for b and c used\n",
    "print(encodeDigits(nil,2),\"\\n\") -- defaults for a and c used\n",
    "print(encodeDigits(),\"\\n\") -- all defaults used\n",
    "print(encodeDigits(1,2,3,4),\"\\n\") -- 4th argument ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3\t1\t\n"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- returning multiple values\n",
    "\n",
    "function divWithRemainder(a,b)\n",
    "    return math.floor(a/b), a%b\n",
    "end\n",
    "\n",
    "d,r = divWithRemainder(10,3)\n",
    "print(d,r)\n",
    "\n",
    "-- (function stuff outside the scope of this tutorial: functions are first class objects, closures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables (more or less the only native data-structure provided by Lua)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables as Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  two : 2\n",
       "  one : 1\n",
       "  3 : three\n",
       "}\n",
       "\n",
       "\t\n",
       "{\n",
       "  two : 2\n",
       "  one : 1\n",
       "  3 : three\n",
       "}\n",
       "\n",
       "\t\n"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- tables can be used as hash-based associative arrays (like python dictionaries)\n",
    "t1 = {} -- construct an empty table\n",
    "t1[\"one\"] = 1\n",
    "t1[\"two\"] = 2\n",
    "t1[3] = \"three\"\n",
    "print(t1, \"\\n\")\n",
    "\n",
    "t2 = {[\"one\"]=1, [\"two\"]=2, [3]=\"three\"} -- constructing a table literal\n",
    "print(t2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\tthree\t\n",
       "\t\n",
       "1\t\n"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- can access string attributes either with brackets, or with dot notation\n",
    "print(t2[\"one\"], t2[3], \"\\n\")\n",
    "print(t2.one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two\t2\t\n",
       "one\t1\t\n",
       "3\tthree\t\n"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- iterating over key, value pairs\n",
    "for k,v in pairs(t1) do\n",
    "    print(k,v)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  two : 2\n",
       "  3 : three\n",
       "}\n",
       "\n",
       "\t\n"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- remove elements from dictionaries by setting to nil\n",
    "t1[\"one\"] = nil\n",
    "print(t1,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables as (ordered) arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : one\n",
       "  2 : two\n",
       "  3 : three\n",
       "}\n",
       "\n",
       "\t\n",
       "{\n",
       "  1 : one\n",
       "  2 : two\n",
       "  3 : three\n",
       "}\n"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- when a table uses only integer keys 1..n, it can also function as an array!\n",
    "-- N.B. Tables (and tensors) are 1-indexed!!!\n",
    "arr = {} -- construct an empty array\n",
    "arr[1] = \"one\"\n",
    "arr[2] = \"two\"\n",
    "arr[3] = \"three\"\n",
    "print(arr,\"\\n\")\n",
    "\n",
    "arr2 = {\"one\", \"two\", \"three\"} -- construct an array literal\n",
    "print(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3\t\n",
       "\t\n",
       "0\t\n",
       "\t\n"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- can get the length of an array by prepending with '#'\n",
    "print(#arr,\"\\n\") \n",
    "\n",
    "--N.B. '#' only works with array-style tables (and not with dictionary-style tables)\n",
    "  -- If you want to get the size of a dictionary in constant time, you need to store it somewhere; gross!\n",
    "ugh = {[\"one\"]=1, [\"two\"]=2}\n",
    "print(#ugh,\"\\n\") -- misleading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : one\n",
       "  2 : two\n",
       "}\n"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- instead to using integer keys to index, can also append to table as follows\n",
    "arr3 = {}\n",
    "table.insert(arr3,\"one\") -- equivalent to t[#t+1] = \"one\"\n",
    "table.insert(arr3,\"two\")\n",
    "print(arr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1\tone\t\n",
       "2\ttwo\t\n",
       "3\tthree\t\n"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- can iterate over an array in order as follows\n",
    "for i, el in ipairs(arr2) do -- ipairs() is like enumerate() in python\n",
    "    print(i, el)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : one\n",
       "  2 : three\n",
       "}\n",
       "\n",
       "\t\n"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- to remove elements from array, use table.remove (which is inefficient)\n",
    "table.remove(arr2,2)\n",
    "print(arr2,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 6.0628e-317   1.0000e+00   1.0000e+00\n",
       "  1.0000e+00   1.0000e+00   1.0000e+00\n",
       "  1.0000e+00   1.0000e+00   1.0000e+00\n",
       "[torch.DoubleTensor of size 3x3]\n",
       "\n",
       "\n",
       "\t\n",
       "(1,.,.) = \n",
       "  0  0\n",
       "  0  0\n",
       "  0  0\n",
       "\n",
       "(2,.,.) = \n",
       "  0  0\n",
       "  0  0\n",
       "  0  0\n",
       "\n",
       "(3,.,.) = \n",
       "  0  0\n",
       "  0  0\n",
       "  0  0\n",
       "[torch.DoubleTensor of size 3x3x2]\n",
       "\n",
       "\n",
       "\t\n",
       "(1,.,.) = \n",
       "  1  1  1\n",
       "\n",
       "(2,.,.) = \n",
       "  1  1  1\n",
       "\n",
       "(3,.,.) = \n",
       "  1  1  1\n",
       "[torch.DoubleTensor of size 3x1x3]\n",
       "\n",
       "\n",
       "\t\n",
       " 0.1739\n",
       "-0.6186\n",
       "[torch.DoubleTensor of size 2]\n",
       "\n",
       "\n",
       "\t\n",
       "(1,1,.,.) = \n",
       "  0.2201\n",
       "[torch.DoubleTensor of size 1x1x1x1]\n",
       "\n",
       "\n",
       "\t\n",
       " 1  1\n",
       " 2  2\n",
       "[torch.DoubleTensor of size 2x2]\n",
       "\n"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--[[ Tensors are multi-dimensional generalizations of arrays/matrices, and are the primary data-structure provided\n",
    "     by Torch (just as arrays are the primary data-structure providedy by Numpy). Tensors are great, and anytime  \n",
    "     you can use them you probably should.\n",
    "\n",
    "     Also check out https://github.com/torch/torch7/blob/master/doc/tensor.md for documentation on Tensor objects,\n",
    "     and https://github.com/torch/torch7/blob/master/doc/maths.md for documentation on mathematical operations\n",
    "     defined on Tensors\n",
    "--]]\n",
    "\n",
    "-- here are some ways of constructing Tensors (of different sizes and dimensions)\n",
    "A = torch.Tensor(3,3) -- an empty 3x3 Tensor (initialized with garbage)\n",
    "B = torch.zeros(3,3,2) -- 3x3x2 Tensor initalized with zeros\n",
    "C = torch.ones(3,1,3)  -- 3x1x3 Tensor initialized with ones\n",
    "D = torch.randn(2) -- 2-vector (still a Tensor) initialized with standard gaussian noise\n",
    "E = torch.rand(1,1,1,1)  -- 1x1x1x1 Tensor initialized with uniform noise\n",
    "F = torch.Tensor({{1,1},{2,2}}) -- 2x2 literal tensor\n",
    "\n",
    "print(A,\"\\n\")\n",
    "print(B,\"\\n\")\n",
    "print(C,\"\\n\")\n",
    "print(D,\"\\n\")\n",
    "print(E,\"\\n\")\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.5437e+07  4.5857e-41 -9.5437e+07\n",
       " 4.5857e-41  1.4013e-45  9.8091e-45\n",
       " 1.7271e-38  0.0000e+00  0.0000e+00\n",
       "[torch.FloatTensor of size 3x3]\n",
       "\n",
       "\n",
       "\t\n",
       "  0  -1  -1\n",
       " -1  -1  -1\n",
       " -1  -1  80\n",
       "[torch.LongTensor of size 3x3]\n",
       "\n",
       "\n",
       "\t\n"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- by default Tensor() gives you a \"real\" Tensor, and you can set whether \"real\" defaults to float or double.\n",
    "-- if you want to explicitly pick one, there are also specialized constructors\n",
    "A = torch.FloatTensor(3,3)\n",
    "print(A,\"\\n\")\n",
    "B = torch.LongTensor(3,3) -- N.B. LongTensors hold integers and are very important; we use them to store indices\n",
    "print(B,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2\t\n",
       "\t\n",
       " 2\n",
       " 3\n",
       "[torch.LongStorage of size 2]\n",
       "\n",
       "6\t\n",
       "\t\n",
       "true\t\n"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- some important ways to get Tensor metadata\n",
    "A = torch.randn(2,3)\n",
    "print(A:dim(),\"\\n\") -- number of dimensions\n",
    "print(A:size()) -- size of each dimension\n",
    "print(A:nElement(),\"\\n\") -- total number of element\n",
    "print(A:isContiguous()) -- does Tensor address a contiguous block of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- can \"view\" a tensor in a different shape without doing any copy\n",
    "a = torch.range(1,6) -- numbers 1 thru 6\n",
    "print(a,\"\\n\")\n",
    "A = a:view(2,3) -- the ':' notation implicitly adds 'self' to a function call (when defined on objects)\n",
    "print(A)\n",
    "\n",
    "-- note view() reshapes along rows (like C and numpy), not along columns (like fortran and R) \n",
    "B = A:view(3,2)\n",
    "print(B,\"\\n\")\n",
    "\n",
    "-- note a, A, and B address the same memory!\n",
    "B:zero() -- zeroes out a tensor\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Sub-Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1  2  3\n",
       " 4  5  6\n",
       "[torch.DoubleTensor of size 2x3]\n",
       "\n",
       "\n",
       "\t\n",
       " 1\n",
       " 2\n",
       " 3\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- index notation allows you to index along the first dimension\n",
    "A = torch.range(1,6):view(2,3)\n",
    "firstRow = A[1]\n",
    "print(A,\"\\n\")\n",
    "print(firstRow)\n",
    "\n",
    "-- this does no memory copy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- select() allows you to index along any dimension\n",
    "firstCol = A:select(2,1) -- select()'s first argument is the desired dimension\n",
    "print(firstCol)\n",
    "-- also does no memory copy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1  2  3\n",
       "[torch.DoubleTensor of size 1x3]\n",
       "\n",
       "\n",
       "\t\n",
       " 2  3\n",
       " 5  6\n",
       "[torch.DoubleTensor of size 2x2]\n",
       "\n",
       " 1  0  0\n",
       " 4  0  0\n",
       "[torch.DoubleTensor of size 2x3]\n",
       "\n"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- instead of accessing a single index, can narrow a Tensor along a chosen dimension\n",
    "firstRow = A:narrow(1,1,1) -- arguments are dim, startIdx, length along dim to extract\n",
    "last2Cols = A:narrow(2,2,2)\n",
    "print(firstRow,\"\\n\")\n",
    "print(last2Cols)\n",
    "\n",
    "-- this also addresses SAME memory as in original tensor\n",
    "last2Cols:zero()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- while narrow() calls can be chained, can also use sub() to narrow along first 4 dimensions\n",
    "A = torch.range(1,6):view(2,3)\n",
    "firstRow = A:sub(1,1) -- arguments are start and stop idx (inclusive) for each dimension (up to 4)\n",
    "last2Cols = A:sub(1,2,2,3) -- using start and stop indices for first two dimensions here\n",
    "bottomRight = A:sub(2,2,3,3)\n",
    "\n",
    "print(firstRow,\"\\n\")\n",
    "print(last2Cols,\"\\n\")\n",
    "print(bottomRight,\"\\n\")\n",
    "\n",
    "-- as above, this addresses same memory as original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1  2  3\n",
       "[torch.DoubleTensor of size 1x3]\n",
       "\n",
       "\n",
       "\t\n",
       " 2  3\n",
       " 5  6\n",
       "[torch.DoubleTensor of size 2x2]\n",
       "\n",
       "\n",
       "\t\n",
       " 6\n",
       "[torch.DoubleTensor of size 1x1]\n",
       "\n",
       "\n",
       "\t\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- instead of using sub() and narrow(), can also specify ranges by indexing with tables\n",
    "firstRow = A[{{1,1},{}}] -- expects table of range-tables, 1 for each dimension; empty table means everything\n",
    "last2Cols = A[{{},{2,3}}] -- note similarity to sub()\n",
    "bottomRight = A[{{2,2},{3,3}}]\n",
    "\n",
    "print(firstRow,\"\\n\")\n",
    "print(last2Cols,\"\\n\")\n",
    "print(bottomRight,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1  2  3\n",
       " 4  5  6\n",
       " 7  8  9\n",
       "[torch.DoubleTensor of size 3x3]\n",
       "\n",
       "\n",
       "\t\n",
       " 1  2  3\n",
       " 7  8  9\n",
       "[torch.DoubleTensor of size 2x3]\n",
       "\n",
       "\n",
       "\t\n",
       " 1  2  3\n",
       " 4  5  6\n",
       " 7  8  9\n",
       "[torch.DoubleTensor of size 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- we can select non-contiguous items along the first dimension using index()\n",
    "A = torch.range(1,9):view(3,3)\n",
    "idxs = torch.LongTensor({1,3}) -- indices are often required to be stored in LongTensors\n",
    "firstAndThirdRows = A:index(1,idxs) -- first argument is the dimension\n",
    "print(A,\"\\n\")\n",
    "print(firstAndThirdRows,\"\\n\")\n",
    "\n",
    "--N.B. index() does a memory copy!\n",
    "firstAndThirdRows:zero()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- can also update a matrix sparsely with indexAdd()\n",
    "A = torch.zeros(3,3)\n",
    "idxs = torch.LongTensor({1,3})\n",
    "U = torch.randn(2,3)\n",
    "A:indexAdd(1,idxs,U) -- U must be of dimension idxs:size(1) x A:size(2)\n",
    "print(A)\n",
    "-- there's also indexFill() and indexCopy(); see the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-place Operations vs. Copying Operations (Very Important!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- for most (mathematical) operations defined on tensors, you will have a choice between allocating new memory for\n",
    "-- the result of the operation, or placing the result in some already-allocated tensor.\n",
    "-- for example, let's consider taking the element-wise absolute value of a tensor A\n",
    "A = torch.randn(3,3)\n",
    "\n",
    "-- if we want to allocate a NEW tensor B s.t. B = abs(A), we do the following\n",
    "B = torch.abs(A) -- in general, using torch.f to call a function f on a tensor will allocate new memory\n",
    "\n",
    "-- let's make sure A has not changed\n",
    "print(A,\"\\n\")\n",
    "print(B,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- suppose instead we have some tensor C lying around that we want to use to store abs(A)\n",
    "C = torch.Tensor(3,3)\n",
    "-- we can use C to store abs(A) as follows\n",
    "C:abs(A) -- recall the ':' notation is short-hand for passing 'self' to a function defined on an object\n",
    "         -- in general, calling X:f(args) will use X's memory to store the result of f(args)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0273  0.2802  1.4295\n",
       " 1.9299  0.0802  0.1325\n",
       " 0.6737  0.8843  0.9304\n",
       "[torch.DoubleTensor of size 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- often it is convenient to use a tensor to store its own result\n",
    "A:abs()\n",
    "-- now A has changed\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- the in-place/copying distinction is important for 2 reasons:\n",
    "-- 1) doing in-place operations is generally much faster than allocating memory repeatedly (e.g., in a loop)\n",
    "-- 2) on the other hand, easy to mess up your data by accidentally doing things in-place\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise mathematical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- many useful elementwise operations are defined, and (as above), can be used in-place or not\n",
    "torch.sqrt(A)\n",
    "A:sqrt()\n",
    "torch.tanh(A)\n",
    "A:tanh()\n",
    "-- can add or multiply by constant scalars\n",
    "A:add(0.5)\n",
    "A:mul(2.6)\n",
    "A:div(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row or Column-wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- can take sum, mean, stddev of rows or columns as follows\n",
    "A = torch.randn(2,3)\n",
    "colSums = A:sum(1) -- sum along first dimension; can also do A:mean(1), A:std(1), etc\n",
    "rowSums = A:sum(2) -- sum along second dimension; can also do A:mean(2), A:std(2), etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 6\n",
       "[torch.DoubleTensor of size 1]\n",
       "\n",
       " 5\n",
       "[torch.LongTensor of size 1]\n",
       "\n"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- torch combines max and argmax() \n",
    "a = torch.range(2,6)\n",
    "maxval, argmax = a:max(1) -- argument specifies dimension\n",
    "print(maxval, argmax)\n",
    "-- can also take min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- adding and multiplying tensors\n",
    "A = torch.randn(2,3)\n",
    "B = torch.randn(2,3)\n",
    "A:add(B) -- puts A+B in A; if want new memory, do torch.add(A,B)\n",
    "B:cmul(A) -- puts ELEMENTWISE multiplication of A and B in B\n",
    "B:cdiv(A) -- puts ELEMENTWISE division B/A in B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- matrix multiplication\n",
    "Ans = torch.Tensor(2,2) -- we'll use this to store an answer\n",
    "-- computes (A B^T) and puts it in Ans\n",
    "Ans:mm(A,B:t())  -- N.B. B:t() transposes B\n",
    "\n",
    "-- dot products\n",
    "dotprod = A:dot(B) -- note, A and B don't need to be vectors (that is, they can have dim > 1)\n",
    "\n",
    "-- matrix-vector products\n",
    "mvAns = torch.Tensor(2) -- stores mv-prod answer\n",
    "v = torch.randn(3) -- 1 dimensional, so a vector\n",
    "mvAns:mv(A,v) -- note could also have done mvAns:view(2,1):mm(A,v:view(3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Add\"-style Linear Algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- instead of overwriting memory w/ a result, you may want to add it to something already computed\n",
    "u = torch.ones(3)\n",
    "v = torch.Tensor(3):fill(2.2)\n",
    "w = torch.Tensor(3):fill(2)\n",
    "c = 3\n",
    "-- compute u = u + c * (v .* w), where .* is elementwise multiplication\n",
    "u:addcmul(c,v,w)\n",
    "print(u,\"\\n\")\n",
    "-- N.B. can also do addcdiv(), which will often be very handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 7  1  1\n",
       " 1  7  1\n",
       " 1  1  7\n",
       "[torch.DoubleTensor of size 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- add-style matrix multiplication\n",
    "Ans = torch.ones(3,3)\n",
    "A = torch.eye(3) -- torch.eye makes an identity matrix\n",
    "B = torch.eye(3):mul(2)\n",
    "Ans:addmm(c,A,B) -- N.B. many more options here; see the documentation!\n",
    "print(Ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
