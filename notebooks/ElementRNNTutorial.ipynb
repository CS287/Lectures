{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll describe the basics of the Element RNN library, which offers implementations and variations of LSTM and GRU recurrent networks. You should check out the documentation and examples at https://github.com/Element-Research/rnn\n",
    "\n",
    "## Review\n",
    "Recall that a recurrent neural network maps a sequence of vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_n$ to a sequence of vectors $\\mathbf{s}_1, \\ldots, \\mathbf{s}_n$ using the recurrence\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{s}_{i} =  R(\\mathbf{x}_{i}, \\mathbf{s}_{i-1}; \\mathbf{\\theta}),\n",
    "\\end{align*}\n",
    "\n",
    "where $R$ is a function parameterized by $\\mathbf{\\theta}$, and we define $\\mathbf{s}_0$ as some initial vector (such as a vector of all zeros).\n",
    "\n",
    "### N.B. For the first part of this notebook we will deal only with \"acceptor\" RNNs\n",
    "* That is, we'll assume we only use the final state $\\mathbf{s}_n$ for making predictions\n",
    "\n",
    "\n",
    "## Element RNN Basics\n",
    "At the heart of the Element RNN library is the abstract class 'AbstractRecurrent', which is designed to allow calling :forward() on each element $\\mathbf{x}_i$ of a sequence (in turn), with the abstract class keeping track of the $\\mathbf{s}_i$ for you. Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0354 -0.2311 -0.0782 -0.0994  0.1091\n",
       "-0.0087 -0.1867 -0.1211 -0.2268  0.2223\n",
       " 0.0590  0.1845 -0.0216 -0.2605  0.1665\n",
       "[torch.DoubleTensor of size 3x5]\n",
       "\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'rnn' -- this imports 'nn' as well, and adds 'rnn' objects to the 'nn' namespace\n",
    "\n",
    "lstm = nn.LSTM(5, 5) -- inherits from AbstractRecurrent\n",
    "data = torch.randn(3, 5) -- a sequence of 3 random vectors\n",
    "outputs = torch.zeros(3, 5)\n",
    "\n",
    "for i = 1, data:size(1) do\n",
    "    outputs[i] = lstm:forward(data[i]) -- note that we don't need to keep track of the s_i\n",
    "end\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, the lstm is able to compute :forward() at each step by keeping track of its states internally. As we discussed in lecture, in an LSTM the states $\\mathbf{s}_i$ comprise the 'hidden state' $\\mathbf{h}_i$ as well as the 'cell' $\\mathbf{c}_i$. In the RNN package's terminology, the $\\mathbf{h}_i$ are known as 'outputs', and the $\\mathbf{c}_i$ as 'cells', and these are stored internally in the nn.LSTM object, as tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 5\n",
       "  2 : DoubleTensor - size: 5\n",
       "  3 : DoubleTensor - size: 5\n",
       "}\n",
       "{\n",
       "  1 : DoubleTensor - size: 5\n",
       "  2 : DoubleTensor - size: 5\n",
       "  3 : DoubleTensor - size: 5\n",
       "}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lstm.outputs)\n",
    "print(lstm.cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lstm.outputs are the same as the outputs tensor we stored manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.0354\n",
       "-0.2311\n",
       "-0.0782\n",
       "-0.0994\n",
       " 0.1091\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n",
       "-0.0087\n",
       "-0.1867\n",
       "-0.1211\n",
       "-0.2268\n",
       " 0.2223\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n",
       " 0.0590\n",
       " 0.1845\n",
       "-0.0216\n",
       "-0.2605\n",
       " 0.1665\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lstm.outputs[1])\n",
    "print(lstm.outputs[2])\n",
    "print(lstm.outputs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the first thing the RNN library gives us is implementations of many of the $R$ functions we're interested in using, such as LSTMs, and GRUs. However, it does much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT\n",
    "\n",
    "To do backpropagation (through time) correctly, we would technically need to loop backwards over the input sequence, as in the following pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- note this doesn't work! just supposed to convey how you might have to implement this...\n",
    "dLdh_i = gradOutForFinalH()\n",
    "dLdc_i = gradOutForFinalC()\n",
    "\n",
    "for i = data:size(1), 1, -1 do\n",
    "    dLdh_iminus1, dLdc_iminus1 = lstm:backward(data[i], {dLdh_i, dLdc_i})\n",
    "    dLdh_i, dLdc_i = dLdh_iminus1, dLdc_iminus1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, however, the Element RNN library can do this for us, using its nn.Sequencer objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencers\n",
    "\n",
    "An nn.Sequencer transforms a module (such as one inheriting from AbstractRecurrent) into a module that can call :forward() on an entire sequence, and :backward() on the entire sequence, thus abstracting away the looping required for backpropagation (through time). \n",
    "\n",
    "In particular, sequencers expect a **table** as input, and also output a table. Let's use an nn.Sequencer on an lstm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 1x5\n",
       "  2 : DoubleTensor - size: 1x5\n",
       "  3 : DoubleTensor - size: 1x5\n",
       "}\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(5, 5)\n",
    "seq_lstm = nn.Sequencer(lstm)\n",
    "inp_table = torch.split(data, 1) -- make a table from our sequence of 3 vectors\n",
    "out_table = seq_lstm:forward(inp_table)\n",
    "print(out_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calling :backward(), an nn.Sequencer expects gradOutput in the same shape as its input, just as every other nn module does. In this case, then, an nn.Sequencer expects gradOutput to be table with an entry for each time step. In particular, gradOutput[i] should contain:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\text{ loss at timestep } i}{\\partial \\mathbf{h}_i}\n",
    "\\end{align*}\n",
    "\n",
    "Since for now we're dealing only with an \"acceptor\" RNN, there is only loss at the final timestep, and so gradOutput[i] is going to be all zeros, when $i < n$, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradOutput = torch.split(torch.zeros(3, 5), 1)\n",
    "-- randomly set final gradOutput\n",
    "gradOutput[#gradOutput] = torch.randn(5) -- note that ordinarily you'd get this from a criterion\n",
    "-- now we can BPTT with a single call!\n",
    "seq_lstm:backward(inp_table, gradOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Tables\n",
    "Since your data will generally not be in tables (but in tensors), it's common to add additional layers to your network to map from tables to tensors and back. For instance, we can create an lstm that takes in a tensor (rather than a table), by using an nn.SplitTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 5\n",
       "  2 : DoubleTensor - size: 5\n",
       "  3 : DoubleTensor - size: 5\n",
       "}\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lstm2 = nn.Sequential():add(nn.SplitTable(1)):add(nn.Sequencer(nn.LSTM(5, 5)))\n",
    "print(seq_lstm2:forward(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an acceptor RNN we only care about the last state, so we can make our final layer a SelectTable (which also simplifies calling :backward(), since it implicitly passes back zeroes for all but the selected table index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.1290\n",
       "-0.0619\n",
       " 0.1116\n",
       "-0.0100\n",
       "-0.0775\n",
       "[torch.DoubleTensor of size 5]\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lstm3 = seq_lstm2:clone()\n",
    "seq_lstm3:add(nn.SelectTable(-1)) -- select the last element in the output table\n",
    "\n",
    "print(seq_lstm3:forward(data))\n",
    "gradOutFinal = gradOutput[#gradOutput] -- note that gradOutFinal is just a tensor\n",
    "seq_lstm3:backward(data, gradOutFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cared about more than the last state of your LSTM, you could add an nn.JoinTable to your network after the Sequencer, which would give a tensor as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "In order to make RNNs fast, it is important to batch. When batching with an Element RNN, time-steps continue to be represented as indices in a table, but this time each element in the table is a **matrix** rather than a vector. In particular, batching occurs along the first dimension (as usual). Thus, a sequence of length 3 with each vector in $\\mathbb{R}^5$ and a batch-size of 2 could be created in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 2x5\n",
       "  2 : DoubleTensor - size: 2x5\n",
       "  3 : DoubleTensor - size: 2x5\n",
       "}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "  1 : DoubleTensor - size: 2x5\n",
       "  2 : DoubleTensor - size: 2x5\n",
       "  3 : DoubleTensor - size: 2x5\n",
       "}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- data representing a sequence of length 3, vectors in R^5, and batch-size of 2\n",
    "batchSequenceDataTbl = {torch.randn(2, 5), torch.randn(2, 5), torch.randn(2, 5)}\n",
    "print(batchSequenceDataTbl)\n",
    "-- do a batched :forward() call\n",
    "print(nn.Sequencer(nn.LSTM(5, 5)):forward(batchSequenceDataTbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember and Forget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use with LookupTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transducer RNNs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
