\documentclass{beamer}
\usepackage{../common_slides}
\usepackage{tikz-qtree}


\title{Recurrent Neural Networks 2}
\date{}
\author{CS 287 \\ (Based on Yoav Goldberg's notes)}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Review: Representation of Sequence}
  \begin{itemize}
  \item   Many tasks in NLP involve sequences
  \[w_1, \ldots, w_n\] 

  \air
   \item Representations as matrix dense vectors $\boldX$ 

  (Following YG, slight abuse of notation)

  \[\boldx_1 =  \boldx^0_1 \boldW^0, \ldots, \boldx_n =\boldx^0_n \boldW^0 \]

  \item Would like fixed-dimensional representation.
  
  \end{itemize}
\end{frame}

\begin{frame}{Review: Sequence Recurrence}
  \begin{itemize}
  \item Can map from dense sequence to dense representation.

  \item $\boldx_1, \ldots, \boldx_n \mapsto \bolds_1, \ldots, \bolds_n$

  \item For all $i \in \{1, \ldots, n \}$ 

      \[\bolds_{i} = R(\bolds_{i-1}, \boldx_i; \theta) \]
    \item $\theta$ is shared by all $R$
  \end{itemize}

  \textbf{Example:} 
  \begin{eqnarray*}
    \bolds_4 &=& R(\bolds_3, \boldx_4) \\ 
             &=& R(R(\bolds_2, \boldx_3), \boldx_4) \\ 
             &=& R(R(R(R(\bolds_0,\boldx_1), \boldx_2), \boldx_3), \boldx_4) \\ 
  \end{eqnarray*}
\end{frame}



\begin{frame}{Review: BPTT (Acceptor)}
  \begin{itemize}
  \item \alert{Run forward propagation}.
  \item \structure{Run backward propagation}.
  \item Update all weights (shared)
  \end{itemize}

    \begin{center}
      \scalebox{0.7}{\Tree [  .\alert<5->{\structure<6->{$\boldy$}} [ .$\alert<4->{\structure<7->{\bolds_n}}$ [ .$\ldots$ [ .$\alert<3->{\structure<8->{\bolds_3}}$ [ .$\alert<2->{\structure<9->{\bolds_2}}$ [ .$\alert<1->{\structure<10->{\bolds_1}}$ $\bolds_0$ $\boldx_1$ ]
      $\boldx_2$ ] $\boldx_3$ ] $\ldots$ ] $\boldx_n$ ] ]}
    \end{center}
\end{frame}

\begin{frame}{Issues}
  \begin{itemize}
  \item Can be inefficient, but batch/GPUs help.
    \air

  \item Model is much deeper than previous approaches.
    \begin{itemize}
    \item This matters a lot, focus of next class.
    \end{itemize}
    \air

  \item Variable-size model for each sentence.
    \begin{itemize}
    \item Have to be a bit more clever in Torch.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Quiz}
  Consider a ReLU version of the Elman RNN with function $R$ defined as
    \[ NN(\boldx, \bolds) = \relu( \bolds \boldW^s  +  \boldx \boldW^x  + \boldb).\]
    
    We  use this RNN with an acceptor architecture over the sequence $\boldx_1, \ldots, \boldx_5$.
    Assume we have computed the gradient for the final layer 
    \[ \frac{\partial L}{\partial \bolds_5} \]   

    What is the symbolic gradient of the previous state
    $ \frac{\partial L}{\partial \bolds_4 }$?    
    
    \air 
    What is the symbolic gradient of the first state
    $ \frac{\partial L}{\partial \bolds_1 }$ ? 

\end{frame}

\begin{frame}{Answer}
  Chain rule, then relu cases, then to indicator notation
  \begin{eqnarray*}
    \frac{\partial L}{\partial s_{4,i} } &=& \sum_{j} \frac{\partial s_{5,j}}{\partial s_{4,i}} \frac{\partial L}{\partial s_{5,j}} \\
     &=&  \sum_{j}
      \begin{cases}
        W^s_{i,j}  \frac{\partial L}{\partial s_{5, j}} &  s_{5, j} > 0\\
        0 &  o.w.\\
      \end{cases} \\
    &=& \sum_{j} \indicator(s_{5, j} > 0) W^s_{i,j}  \frac{\partial L}{\partial s_{5, j}} 
  \end{eqnarray*}
\end{frame}


\begin{frame}{Answer}
  Multiple applications of Chain rule, combine relu cases.
  \begin{eqnarray*}
    \frac{\partial L}{\partial s_{1,j_1}} &=& \sum_{j_2} \ldots \sum_{j_5} \frac{\partial s_{5,j_5}}{\partial s_{4,j_4}} \frac{\partial L}{\partial s_{5,j_5}}\\
    &=& \sum_{j_2} \ldots \sum_{j_5} \indicator(s_{2, j_2} > 0\land \ldots  \land s_{5, j_5} > 0) W^s_{j_1,j_2} \ldots W^s_{j_4,j_5}  \frac{\partial L}{\partial s_{5, j_5}}  \\ 
    &=& \sum_{j_2 \ldots j_5} \prod_{k=2}^5 \indicator(s_{k, j_k} > 0)  W^s_{j_{k-1},j_k}  \frac{\partial L}{\partial s_{5, j_5}}  \\ 
  \end{eqnarray*}
\end{frame}

\begin{frame}{The Promise of RNNs}
  \begin{itemize}
  \item We hope to learn a model with memory.
    \air 
  \item For acceptors this means long-range interaction.

  \air 
  \texttt{How can you not see this movie?}
  \air 

  \texttt{You should not see this movie.}

\item Memory interaction here is at  $\bolds_1$, but gradient signal is at $\bolds_n$   
  \end{itemize}
\end{frame}

\begin{frame}{Vanishing Gradients}
  \begin{itemize}
  \item Gradients at early layers go through many squashing layers.
    \air

  \item For instance consider quiz with  hardtanh
    \[      \sum_{j_2 \ldots j_5} \prod_{k=2}^5 \indicator(1 > s_{k, j_k} > 0)  W^s_{j_{k-1},j_k}  \frac{\partial L}{\partial s_{5, j_5}}    \]

    \air
  \item The indicator term causes a tendency towards \textit{vanishing gradients}.
   \air

  \item If this occurs, model cannot learn long-term dependencies.
 
  \end{itemize}
  
\end{frame}

\begin{frame}{LSTM (Hochreiter and Schmidhuber, 1997)}
  \begin{center}
    \includegraphics[width=\textwidth]{LSTM3-chain}
  \end{center}
\end{frame}

\begin{frame}
  \begin{eqnarray*}
    R(\bold \bolds_{i-1}, \boldx_i) &=& [\boldc_i, \boldh_i]  \\
    \boldc_i &=& \boldj \odot \boldi  + \bold f \odot \bold c_{i-1}    \\
    \boldh_i &=& \tanh(\boldc_i) \alert{\odot \boldo}\\ 
    \boldi &=& \tanh(\boldx \boldW^{xi} + \boldh_{i-1}\boldW^{hi} + \boldb^i) \\
    \boldj &=& \sigma(\boldx \boldW^{xj} + \boldh_{i-1}\boldW^{hj} + \boldb^j) \\
    \boldf &=& \sigma(\boldx \boldW^{xf} + \boldh_{i-1}\boldW^{hf} + \boldb^f) \\
    \boldo &=& \tanh(\boldx \boldW^{xo} + \boldh_{i-1}\boldW^{ho} + \boldb^o) \\
  \end{eqnarray*}
  \begin{itemize}
  \item $\boldf$; forget gate
  \item $\boldi$; input gate
  \item $\boldc$; cell state
  \item $\boldh$; hidden state
  \item $\boldo$; output gate (not very important)
  \end{itemize}
\end{frame}

\begin{frame}
  
\end{frame}

\section{Highway Networks}

\begin{frame}{Deep Networks}
  \begin{itemize}
  \item This same issue occurs in deep MLPs.
  \end{itemize}
  \[NN_{layer}(\boldx) = \relu(\boldx \boldW^1 + \boldb^1)  \]
      \begin{center}
      \scalebox{0.7}{\Tree [ .$\boldh_n$ [ .$\boldh_{n-1}$ [ .$\ldots$  [ .$\boldh_2$ [ .$\boldh_1$ $\boldx$ ] ] ]  ]  ] }
    \end{center}
\end{frame}

\begin{frame}{Thought Experiment: Additive Skip-Connections}
  \[NN_{sl1}(\boldx) = \frac{1}{2} \relu(\boldx \boldW^1 + \boldb^1) + \frac{1}{2} \boldx \]
      \begin{center}
      \scalebox{0.7}{
        \begin{tikzpicture}
          \Tree [ .\node{$\boldh_n$}; [ .\node{$\boldh_{n-1}$}; [
          .$\ldots$ [ .\node(hc){$\boldh_3$}; [ .\node(hb){$\boldh_2$}; [
          .\node(ha){$\boldh_1$}; \node(x){$\boldx$}; ] ] ] ] ] ]
          \draw (x) edge[bend left] (hb);
          \draw (ha) edge[bend right] (hc);
        \end{tikzpicture}
      }
    \end{center}
  
\end{frame}

\begin{frame}{Exercise}
  Original model has same gradient issue as with RNN.

  \[ \frac{\partial L}{\partial h_{n-1,j_{n-1}} }   =  \sum_{j_n}  \indicator(h_{n, j_n} > 0) W_{j_{n-1},j_n}  \frac{\partial L}{\partial h_{n, j_n}}     \]


  Exercise: What happens to the gradient of $n-1$ with skip-connections ?
\end{frame}


\begin{frame}{Exercise}
  We now have the average of two terms. One with no saturation condition.
  \air 
  
  \begin{eqnarray*}
     \frac{\partial L}{\partial h_{n-1,j_{n-1}} } & = & \frac{1}{2} (\sum_{j_n}   \indicator(h_{n, j_n} > 0) W_{j_{n-1},j_n} \frac{\partial L}{\partial h_{n, j_n}}) + \frac{1}{2} ( h_{n-1, j_{n-1}} \frac{\partial L}{\partial h_{n, j_{n-1}}})  \\ 
  \end{eqnarray*}

 

\end{frame}

\begin{frame}{Thought Experiment: Dynamic Skip-Connections}
  \begin{eqnarray*}
    NN_{sl2}(\boldx) &=& (1-t) \relu(\boldx \boldW^1 + \boldb^1) + t \boldx \\
    t &=& \sigma(\boldx \boldW^t + b^t) \\
    \boldW^t &\in& \reals^{\din \times 1} 
  \end{eqnarray*}


      \begin{center}
      \scalebox{0.7}{
        \begin{tikzpicture}
          \Tree [ .\node{$\boldh_n$}; [ .\node{$\boldh_{n-1}$}; [
          .$\ldots$ [ .\node(hc){$\boldh_3$}; [ .\node(hb){$\boldh_2$}; [
          .\node(ha){$\boldh_1$}; \node(x){$\boldx$}; ] ] ] ] ] ]
          \draw (x) edge[bend left] (hb);
          \draw (ha) edge[bend right] (hc);
        \end{tikzpicture}
      }
    \end{center}
\end{frame}

\begin{frame}{Thought Experiment: Dynamic Skip-Connections}
  \begin{eqnarray*}
    NN_{sl2}(\boldx) &=& (1-t) \relu(\boldx \boldW^1 + \boldb^1) + t \boldx \\
    t &=& \sigma(\boldx \boldW^t + b^t) \\
    \boldW^t &\in& \reals^{\dhid \times 1} 
  \end{eqnarray*}


  The $t$ values are saved on the forward pass.
  \begin{eqnarray*}
     \frac{\partial L}{\partial h_{n-1,j_{n-1}} }  =  &(1-t)& (\sum_{j_n}   \indicator(h_{n, j_n} > 0) W_{j_{n-1},j_n} \frac{\partial L}{\partial h_{n, j_n}}) \\
    + &t& ( h_{n-1, j_{n-1}} \frac{\partial L}{\partial h_{n, j_{n-1}}})  \\ 
  \end{eqnarray*}
\end{frame}


\begin{frame}{Thought Experiment: Dynamic Skip-Connections}
  \begin{itemize}
  \item Note: $\boldW^t$ is also receiving gradients through the sigmoid!
    \air
  \item Learn how to trade-off skipping versus deep layers.
    \air
  \item (Backprop is fun.)
  \end{itemize}
  \begin{eqnarray*}
    NN_{sl2}(\boldx) &=& (1-t) \relu(\boldx \boldW^1 + \boldb^1) + t \boldx \\
    t &=& \sigma(\boldx \boldW^t + b^t) \\
    \boldW^t &\in& \reals^{\dhid \times 1} 
  \end{eqnarray*}
\end{frame}

\begin{frame}{Highway Network (Srivastava et al., 2015)}
  \begin{eqnarray*}
    NN_{highway}(\boldx) &=& (1-\boldt) \odot \tilde{\boldh} + \boldt \odot \boldx \\
    \tilde{\boldh} &=& \relu(\boldx \boldW^1 + \boldb^1) \\
    \boldt &=& \sigma(\boldx \boldW^t + \boldb^t) \\
    \boldW^t &\in& \reals^{\dhid \times \dhid} \\
    \boldb^t &\in& \reals^{1 \times \dhid} \\
  \end{eqnarray*}
  \begin{itemize}
  \item $\tilde{\boldh}$; \textit{transform} (e.g. standard MLP layer)
  \item $\boldt$; \textit{carry} (dimension-specific dynamic skipping)
  \end{itemize}
\end{frame}

\begin{frame}{Highway Gradients}
    The $\boldt$ values are saved on the forward pass.
  \begin{eqnarray*}
     \frac{\partial L}{\partial h_{n-1,j_{n-1}} }  =  && (\sum_{j_n} (1-t_{j_n})  \indicator(h_{n, j_n} > 0) W_{j_{n-1},j_n} \frac{\partial L}{\partial h_{n, j_n}}) \\
    + &t_{j_{n-1}}& ( h_{n-1, j_{n-1}} \frac{\partial L}{\partial h_{n, j_{n-1}}})  \\ 
  \end{eqnarray*}
\end{frame}


\begin{frame}{Gating}
  \begin{itemize}
  \item This is known as the \textit{gating} operation
    \[ \boldt \odot \boldx \]
    \air 

  \item Allows vector $\boldt$ to mask or gate $\boldx$.
    \air

  \item True gating would have $\boldt \in \{0,1\}^{\dhid}$

    \air 
  \item Approximate with the sigmoid,
    \[\boldt = \sigma( \boldW^t \boldx \boldb) \] 
  \end{itemize}
\end{frame}

\section{GRU}

\begin{frame}{Back To RNNs}
  \begin{itemize}
  \item Acceptor RNNs are deep networks with shared weights.
    \air 

  \item Can replace Elman layer with modified highway layer. 

    \begin{eqnarray*}
      R(\bolds_{i-1}, \boldx_i) &=& (1-\boldt) \odot \tilde{\boldh} + \boldt \odot \bolds_{i-1} \\
      \tilde{\boldh} &=& \tanh(\boldx \boldW^x + \bolds_{i-1} \boldW^s + \boldb)  \\
      \boldt &=& \sigma(\boldx \boldW^{xt} + \bolds_{i-1}\boldW^{st} + \boldb^t) \\
      \boldW^{xt}, \boldW^{x} &\in& \reals^{\din \times \dhid} \\
      \boldW^{st}, \boldW^{s} &\in& \reals^{\dhid \times \dhid} \\
      \boldb^t, \boldb &\in& \reals^{1 \times \dhid} \\
    \end{eqnarray*}
  \end{itemize}
\end{frame}

\begin{frame}{Dynamic Connections for RNN}
  \begin{center}
    \begin{tikzpicture}
      \Tree [ .$\bolds_n$ [ .$\ldots$ [ .\node(sd){$\bolds_4$}; [ .\node(sc){$\bolds_3$}; [ .\node(sb){$\bolds_2$}; [ .\node(sa){$\bolds_1$}; \node(sz){$\bolds_0$}; $\boldx_1$ ]
      $\boldx_2$ ] $\boldx_3$ ] $\boldx_4$ ] $\ldots$ ] $\boldx_n$ ];
      \draw (sz) edge[dashed, bend left] (sb);
      \draw (sa) edge[dashed, bend left] (sc);
      \draw (sb) edge[dashed, bend left] (sd);
    \end{tikzpicture}
  \end{center}
\end{frame}

\begin{frame}{Final Idea: Stopping flow}
  \begin{itemize}
  \item For many tasks, it is useful to halt propagation.
    \air
  \item Can do this by applying a reset/forget gate. 
    \air 

    \begin{eqnarray*}
      \tilde{\boldh} &=& \tanh(\boldx \boldW^x + (\boldr \odot \bolds_{i-1})  \boldW^s + \boldb)  \\
      \boldr &=& \sigma(\boldx \boldW^{xr} + \bolds_{i-1}\boldW^{sr} + \boldb^r) \\
    \end{eqnarray*}
  \item Example: Language Modeling
    
  \end{itemize}
\end{frame}

\begin{frame}{Gated Recurrent Unit (GRU) (Cho et al 2014)}
    \begin{eqnarray*}
      R(\bolds_{i-1}, \boldx_i) &=& (1-\boldt) \odot \tilde{\boldh} + \boldt \odot \bolds_{i-1} \\
      \tilde{\boldh} &=& \tanh(\boldx \boldW^x + (\boldr \odot \bolds_{i-1})  \boldW^s + \boldb)  \\
      \boldr &=& \sigma(\boldx \boldW^{xr} + \bolds_{i-1}\boldW^{sr} + \boldb^r) \\
      \boldt &=& \sigma(\boldx \boldW^{xt} + \bolds_{i-1}\boldW^{st} + \boldb^t) \\
      \boldW^{xt}, \boldW^{xr}, \boldW^{x} &\in& \reals^{\din \times \dhid} \\
      \boldW^{st}, \boldW^{sr}, \boldW^{s} &\in& \reals^{\dhid \times \dhid} \\
      \boldb^t, \boldb &\in& \reals^{1 \times \dhid} \\
    \end{eqnarray*}

    \begin{itemize}
    \item $\boldt$; dynamic skip-connections
    \item $\boldr$; reset gating
    \item $\bolds$; hidden state
    \item $\tilde{\boldh}$; gated MLP
    \end{itemize}
\end{frame}

\section{LSTM}

\begin{frame}{LSTM}
  
\end{frame}

\begin{frame}{LSTMs Development}
    \begin{eqnarray*}
      R(\bolds_{i-1}, \boldx_i) &=& [\boldc_i, \boldh_i]  \\
      \alert{\boldh_i} &=& \tanh(\boldc_i) \\ 
      \boldc_i &=& (1-\boldt) \odot \tilde{\boldh}  + \boldt \odot \bold c_{i-1}    \\
      \tilde{\boldh} &=& \tanh(\boldx \boldW^{xi} + \boldh_{i-1}\boldW^{hi} + \boldb^i) \\
      \boldt &=& \sigma(\boldx \boldW^{xt} + \boldh_{i-1}\boldW^{ht} + \boldb^t) \\
    \end{eqnarray*}
  The state $\bolds_i$ is made of 2 components :
  \begin{itemize}
  \item $\boldc_i$; cell
  \item $\boldh_i$; hidden
  \end{itemize}

\end{frame}

\begin{frame}{LSTM Development: Input and Forget Gates}
    \begin{eqnarray*}
      R(\boldc_{i-1}, \boldx_i) &=& [\boldc_i, \boldh_i]  \\
      \boldh_i &=& \tanh(\boldc_i) \\ 
      \boldc_i &=& \alert{\boldj \odot \tilde{\boldh}  + \boldf \odot \bold c_{i-1} } \\
      \tilde{\boldh} &=& \tanh(\boldx \boldW^{xi} + \boldh_{i-1}\boldW^{hi} + \boldb^i) \\
      \boldj &=& \sigma(\boldx \boldW^{xj} + \boldh_{i-1}\boldW^{hj} + \boldb^j) \\
      \boldf &=& \sigma(\boldx \boldW^{xf} + \boldh_{i-1}\boldW^{hf} + \boldb^f) \\
    \end{eqnarray*}

    No longer a convex combination.
    \begin{itemize}
    \item $\boldc_i$; cell
    \item $\boldh_i$; hidden
    \item $\boldj$; input gate
    \item $\boldf$; forget gate
    \end{itemize}
\end{frame}

% \begin{frame}{Long Short-Term Memory (without output)}
%     \begin{eqnarray*}
%       R(\bold \bolds_{i-1}, \boldx_i) &=& [\boldc_i, \boldh_i]  \\
%       \boldc_i &=& \boldj \odot \boldi  + \bold f \odot \bold c_{i-1}    \\
%       \boldh_i = \tanh(\boldc_i) %\odot \boldo
%       \boldi &=& \tanh(\boldx \boldW^{xi} + \boldh_{i-1}\boldW^{hi} + \boldb^i) \\
%       \boldj &=& \sigma(\boldx \boldW^{xj} + \boldh_{i-1}\boldW^{hj} + \boldb^j) \\
%       \boldf &=& \sigma(\boldx \boldW^{xf} + \boldh_{i-1}\boldW^{hf} + \boldb^f) \\
%       % \boldo &=& \tanh(\boldx \boldW^{xo} + \boldh_{i-1}\boldW^{ho} + \boldb^o) \\
%     \end{eqnarray*}
%     \begin{itemize}
%     \item $\boldf$; forget gate
%     \item $\boldi$; input gate
%     \item $\boldo$; output gate
%     \item $\boldc$; cell state
%     \item $\boldh$; hidden state
%     \end{itemize}
% \end{frame}


\begin{frame}{Long Short-Term Memory}
    \begin{eqnarray*}
      R(\bold \bolds_{i-1}, \boldx_i) &=& [\boldc_i, \boldh_i]  \\
      \boldc_i &=& \boldj \odot \boldi  + \bold f \odot \bold c_{i-1}    \\
      \boldh_i &=& \tanh(\boldc_i) \alert{\odot \boldo}\\ 
      \boldi &=& \tanh(\boldx \boldW^{xi} + \boldh_{i-1}\boldW^{hi} + \boldb^i) \\
      \boldj &=& \sigma(\boldx \boldW^{xj} + \boldh_{i-1}\boldW^{hj} + \boldb^j) \\
      \boldf &=& \sigma(\boldx \boldW^{xf} + \boldh_{i-1}\boldW^{hf} + \boldb^f) \\
      \boldo &=& \tanh(\boldx \boldW^{xo} + \boldh_{i-1}\boldW^{ho} + \boldb^o) \\
    \end{eqnarray*}
    \begin{itemize}
    \item $\boldf$; forget gate
    \item $\boldi$; input gate
    \item $\boldc$; cell state
    \item $\boldh$; hidden state
    \item $\boldo$; output gate (not very important)
    \end{itemize}
\end{frame}

\begin{frame}
  \begin{center}
    \includegraphics[width=\textwidth]{LSTM3-chain}
  \end{center}

\end{frame}



\end{document}
