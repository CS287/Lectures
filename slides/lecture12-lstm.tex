\documentclass{beamer}
\usepackage{../common_slides}
\usepackage{tikz-qtree}


\title{Recurrent Neural Networks 2}
\date{}
\author{CS 287 \\ (Based on Yoav Goldberg's notes)}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Review: Representation of Sequence}
  \begin{itemize}
  \item   Many tasks in NLP involve sequences
  \[w_1, \ldots, w_n\] 

  \air
   \item Representations as matrix dense vectors $\boldX$ 

  (Following YG, slight abuse of notation)

  \[\boldx_1 =  \boldx^0_1 \boldW^0, \ldots, \boldx_n =\boldx^0_n \boldW^0 \]

  \item Would like fixed-dimensional representation.
  
  \end{itemize}
\end{frame}

\begin{frame}{Review: Sequence Recurrence}
  \begin{itemize}
  \item Can map from dense sequence to dense representation.

  \item $\boldx_1, \ldots, \boldx_n \mapsto \bolds_1, \ldots, \bolds_n$

  \item For all $i \in \{1, \ldots, n \}$ 

      \[\bolds_{i} = R(\bolds_{i-1}, \boldx_i; \theta) \]
    \item $\theta$ is shared by all $R$
  \end{itemize}

  \textbf{Example:} 
  \begin{eqnarray*}
    \bolds_4 &=& R(\bolds_3, \boldx_4) \\ 
             &=& R(R(\bolds_2, \boldx_3), \boldx_4) \\ 
             &=& R(R(R(R(\bolds_0,\boldx_1), \boldx_2), \boldx_3), \boldx_4) \\ 
  \end{eqnarray*}
\end{frame}



\begin{frame}{Review: BPTT (Acceptor)}
  \begin{itemize}
  \item \alert{Run forward propagation}.
  \item \structure{Run backward propagation}.
  \item Update all weights (shared)
  \end{itemize}

    \begin{center}
      \scalebox{0.7}{\Tree [  .\alert<5->{\structure<6->{$\boldy$}} [ .$\alert<4->{\structure<7->{\bolds_n}}$ [ .$\ldots$ [ .$\alert<3->{\structure<8->{\bolds_3}}$ [ .$\alert<2->{\structure<9->{\bolds_2}}$ [ .$\alert<1->{\structure<10->{\bolds_1}}$ $\bolds_0$ $\boldx_1$ ]
      $\boldx_2$ ] $\boldx_3$ ] $\ldots$ ] $\boldx_n$ ] ]}
    \end{center}
\end{frame}

\begin{frame}{Issues}
  \begin{itemize}
  \item Can be inefficient, but batch/GPUs help.
    \air

  \item Model is much deeper than previous approaches.
    \begin{itemize}
    \item This matters a lot, focus of next class.
    \end{itemize}
    \air

  \item Variable-size model for each sentence.
    \begin{itemize}
    \item Have to be a bit more clever in Torch.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Quiz}
  Consider a ReLU version of the Elman RNN with function $R$ defined as
    \[ NN(\boldx, \bolds) = \relu( \bolds \boldW^s  +  \boldx \boldW^x  + \boldb).\]
    
    We  use this RNN with an acceptor architecture over the sequence $\boldx_1, \ldots, \boldx_5$.
    Assume we have computed the gradient for the final layer 
    \[ \frac{\partial L}{\partial \bolds_5} \]   

    What is the symbolic gradient of the previous state
    $ \frac{\partial L}{\partial \bolds_4 }$?    
    
    \air 
    What is the symbolic gradient of the first state
    $ \frac{\partial L}{\partial \bolds_1 }$ ? 

\end{frame}

\begin{frame}{Answer}
  Chain rule, then relu cases, then to indicator notation
  \begin{eqnarray*}
    \frac{\partial L}{\partial s_{4,i} } &=& \sum_{j} \frac{\partial s_{5,j}}{\partial s_{4,i}} \frac{\partial L}{\partial s_{5,j}} \\
     &=&  \sum_{j}
      \begin{cases}
        W^s_{i,j}  \frac{\partial L}{\partial s_{5, j}} &  s_{5, j} > 0\\
        0 &  o.w.\\
      \end{cases} \\
    &=& \sum_{j} \indicator(s_{5, j} > 0) W^s_{i,j}  \frac{\partial L}{\partial s_{5, j}} 
  \end{eqnarray*}
\end{frame}


\begin{frame}{Answer}
  Multiple applications of Chain rule, combine relu cases.
  \begin{eqnarray*}
    \frac{\partial L}{\partial s_{1,i}} &=& \sum_{j_2} \ldots \sum_{j_5} \frac{\partial s_{5,j_5}}{\partial s_{4,j_4}} \frac{\partial L}{\partial s_{5,j_5}}\\
    &=& \sum_{j_2} \ldots \sum_{j_5} \indicator(s_{2, j_2} > 0\land \ldots  \land s_{5, j_5} > 0) W^s_{i,j_2} \ldots W^s_{j_4,j_5}  \frac{\partial L}{\partial s_{5, j}}  \\ 
  \end{eqnarray*}
\end{frame}

\begin{frame}{The Promise of RNNs}
  \begin{itemize}
  \item We hope to learn a model with memory.
    \air 
  \item For acceptors this means long-range interaction.

  \air 
  \texttt{How can you not see this movie?}
  \air 

  \texttt{You should not see this movie.}

\item Memory interaction here is at  $\bolds_1$, but gradient signal is at $\bolds_n$   
  \end{itemize}
\end{frame}

\begin{frame}{Vanishing Gradients}
  \begin{itemize}
  \item Gradients at early layers go through many squashing layers.
    \air

  \item For instance consider quiz with  hardtanh
    \[ \sum_{j_2} \ldots \sum_{j_5} \indicator((0< s_{2, j_2} < 1)\land \ldots  \land (0<s_{5, j} < 1)) W^s_{i,j_2} \ldots W^s_{j_4,j_5}   \frac{\partial L}{\partial s_{5, j}} \]

    \air
  \item The indicator term causes a tendency towards \textit{vanishing gradients}.
   \air

  \item If this occurs, model cannot learn long-term dependencies.
 
  \end{itemize}
  
\end{frame}

\section{Highway Networks}

\begin{frame}{Deep Networks}
  \begin{itemize}
  \item This same issue occurs in deep MLPs.
  \end{itemize}
  \[NN_{layer}(\boldx) = \relu(\boldx \boldW^1 + \boldb^1)  \]
      \begin{center}
      \scalebox{0.7}{\Tree [ .$\boldh_n$ [ .$\boldh_{n-1}$ [ .$\ldots$  [ .$\boldh_2$ [ .$\boldh_1$ $\boldx$ ] ] ]  ]  ] }
    \end{center}
\end{frame}

\begin{frame}{Thought Experiment: Additive Skip-Connections}
  \[NN_{sl1}(\boldx) = \frac{1}{2} \relu(\boldx \boldW^1 + \boldb^1) + \frac{1}{2} \boldx \]
      \begin{center}
      \scalebox{0.7}{
        \begin{tikzpicture}
          \Tree [ .\node{$\boldh_n$}; [ .\node{$\boldh_{n-1}$}; [
          .$\ldots$ [ .\node(hc){$\boldh_3$}; [ .\node(hb){$\boldh_2$}; [
          .\node(ha){$\boldh_1$}; \node(x){$\boldx$}; ] ] ] ] ] ]
          \draw (x) edge[bend left] (hb);
          \draw (ha) edge[bend right] (hc);
        \end{tikzpicture}
      }
    \end{center}
  
\end{frame}

\begin{frame}{Exercise}
  What Happens to the Gradient?

  \[ \frac{\partial L}{\partial h_{1,i}} &=& \sum_{j_2} \ldots \sum_{j_5} \indicator(h_{2, j_2} > 0\land \ldots  \land h_{5, j_5} > 0) W_{i,j_2} \ldots W_{j_4,j_5}  \frac{\partial L}{\partial h_{5, j}}  \\ 

\end{frame}

\begin{frame}{Thought Experiment: Dynamic Skip-Connections}
  \begin{eqnarray*}
    NN_{sl1}(\boldx) &=& (1-t) \relu(\boldx \boldW^1 + \boldb^1) + t \boldx \\
    t &=& \sigma(\boldx \boldW^t + b^t) \\
    \boldW^t &\in& \reals^{\din \times 1} 
  \end{eqnarray*}


      \begin{center}
      \scalebox{0.7}{
        \begin{tikzpicture}
          \Tree [ .\node{$\boldh_n$}; [ .\node{$\boldh_{n-1}$}; [
          .$\ldots$ [ .\node(hc){$\boldh_3$}; [ .\node(hb){$\boldh_2$}; [
          .\node(ha){$\boldh_1$}; \node(x){$\boldx$}; ] ] ] ] ] ]
          \draw (x) edge[bend left] (hb);
          \draw (ha) edge[bend right] (hc);
        \end{tikzpicture}
      }
    \end{center}

\end{frame}

\section{GRU}

\begin{frame}
  
\end{frame}

\begin{frame}{LSTMs}
  
\end{frame}


\end{document}
